# About Me

Postdoctoral fellow working on computational aspects of differential, geometric, and algebraic structures (i.e., probability distributions and matrices). My research so far has mostly focused on geometric methods for **numerical optimization** and **approximate inference** in machine learning.


For natural-gradient (NG) methods, please see 
* Improving optimizers Shampoo and SOAP via Riemannian/proximal gradient descent (ICLR 2026): [Paper](https://arxiv.org/abs/2509.03378), [Code](https://github.com/yorkerlin/KL-Methods)
* Adaptive gradient methods as NGD (ICML 2024): [Paper](https://arxiv.org/abs/2402.03496), [Code](https://github.com/yorkerlin/remove-the-square-root)
* Structured NG descent for deep learning (ICML 2023, ICML 2024): [Manifold View](https://arxiv.org/abs/2302.09738)  [Code 2023](https://github.com/yorkerlin/StructuredNGD-DL);  [Bayesian View](https://arxiv.org/abs/2312.05705) [Code 2024](https://github.com/f-dangel/singd)
* Structured NG descent (ICML 2021):[Paper](https://arxiv.org/abs/2102.07405), [Blog]({{ site.baseurl }}{% post_url 2021-07-05-GeomProj01 %})
* Riemannian gradient descent (ICML 2020): [Paper](https://arxiv.org/abs/2002.10060), [Code](https://github.com/yorkerlin/iBayesLRule)
* NG descent for exponential-family mixtures (ICML 2019): [Paper](https://arxiv.org/abs/1906.02914), [Code](https://github.com/yorkerlin/VB-MixEF)
* NG descent for Bayesian deep learninng (ICML 2018): [Paper](https://arxiv.org/abs/1806.04854), [Code](https://github.com/emtiyaz/vadam)
* NG variational inference for non-conjugate models (AI&Stats 2017): [Paper](https://arxiv.org/abs/1703.04265), [Code](https://github.com/emtiyaz/cvi)


For an introduction to NG methods, see my [Blog Posts]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}).

For more publications, see my [Google Scholar](https://scholar.google.com/citations?user=sGl6muoAAAAJ&hl=en) page.

<!--I review papers from conferences ([ICML](https://icml.cc/), [NeurIPS](https://nips.cc/), [ICLR](https://iclr.cc/), [AI&Stats](https://aistats.org/)), journals ([JMLR](https://www.jmlr.org/), [Information Geometry (Springer)](https://www.springer.com/journal/41884)), and workshops ([Optimization for Machine Learning](https://opt-ml.org/), [Advances in Approximate Bayesian Inference](http://approximateinference.org/)).-->

## Research Interests

I am interested in exploiting (hidden) structures and symmetries in machine learning with a focus on practical and numerical methods for optimization and statistical inference.

<!--## News-->

<!--{% include news.html %}-->
<!--[Click here for all news](/news/)-->

<!--* Designing adaptive non-diagonal methods via Riemannian gradient descent (Arxiv 2025): [paper](https://arxiv.org/abs/2502.06268)-->
