---
title: 'Part III: Invarinace of Natural-Gradients'
date: 2021-11-01
permalink: /posts/2021/11/Geomopt03/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand the invariance of natural-gradients

We will give an informal introduction with a focus on high level of ideas.



# Parameter Transform and Invariance
------




Recall that a Riemannian gradient is a parametric representation of the tangent direction of a curve in the manifold.
Since a curve and its tangent direction are geometric obejects, they should be invariant to the choice of parametrization.
In other words, geometric properties should be persevered in any valid coordinate system.


We now discuss one **geometric property**: the directional derivative should be same if we perform a parameter transform.

In the previous, we consider a point $\mathbf{x}_0$ in a manifold $\mathcal{M}$, a (1-dimensional) curve $\gamma(t)$, and a smooth scalar function $h: \mathcal{M} \to \mathcal{R}$.

Given an intrinsic parametrization $\tau$ containing the point, we consider the following parametric representations.
 
|   geometric object   |   parametric representation  |
|:------------|:-------------:|
| point `$\mathbf{x}_0$` |  `$\tau_0$`   | 
| curve  `$\gamma(t)$`  | `$\gamma_\tau(t) $`  | 
| function  `$h$`  | `$h_\tau(\tau) $`  |   




We want the following indentity holds for any two (intrinsic) parametrizations $\tau$ and $\lambda$.
`$$
\begin{aligned}
h(\gamma(t)) = h_\tau(\gamma_\tau(t)) = h_\lambda(\gamma_\lambda(t))
\end{aligned}
$$`

From the above expression, we can see that directional derivatives should be the same. 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} = \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} 
\end{aligned}\tag{3}\label{3}
$$`

In the previous section, we have shown that 
`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &=  [\nabla h_\tau(\mathbf{\tau}_0)  ]^T  \frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}   \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & = [\nabla h_\lambda(\mathbf{\lambda}_0) ]^T  \frac{d {\gamma}_\lambda(t) }{d t} \Big|_{t=0}  
\end{aligned}
$$` where $\nabla$ is the standard (coordinate) derivate.


We use the following the notations to simplify the expressions.

|  Notation   |   Meanings  |
|:------------|:-------------:|
| Euclidean gradient `$(g_\tau)_i$`  |  $i$-th entry  under parametrization $\tau$  | 
| Riemannian gradient `$(\hat{g}_\tau)^j$` |  $j$-th entry under parametrization $\tau$  | 
| Parameter `$\tau^j$` |  $j$-th parameter under parametrization   $\tau$  | 
   
The derivational derivatives can be re-expressed as

`$$
\begin{aligned}
 \frac{d h_\tau({\gamma}_\tau(t)) }{d t} \Big|_{t=0} &= \sum_{i}  (g_\tau)_i  (\hat{g}_\tau)^i  = {\mathbf{g}_\tau} \hat{\mathbf{g}}_\tau \\ 
 \frac{d h_\lambda({\gamma}_\lambda(t)) }{d t} \Big|_{t=0} & =\sum_{i}  (g_\lambda)_i (\hat{g}_\lambda)^i =  {\mathbf{g}}_\lambda \hat{\mathbf{g}}_\lambda
\end{aligned}\tag{4}\label{4}
$$`
where `$\mathbf{g}_\lambda$` and `$\mathbf{g}_\tau$` (e.g.,  `$\mathbf{g}_\tau=(\nabla h_\tau(\tau_0) )^T$`) are **row** vectors (Euclidean gradients) while `$\hat{\mathbf{g}}_\lambda$` and `$\hat{\mathbf{g}}_\tau$` (e.g.,`$\hat{\mathbf{g}}_\tau=\mathbf{v}_\tau(\tau_0) =\frac{d \gamma_\tau(0) }{d t}$`) are **column** vectors (Riemannian gradients). 

By `$\eqref{3}$` and `$\eqref{4}$`, we have  one invariance property for the directional derivative.
`$$
\begin{aligned}
 \mathbf{g}_\tau \hat{\mathbf{g}}_\tau  =  \mathbf{g}_\lambda \hat{\mathbf{g}}_\lambda 
\end{aligned}\tag{5}\label{5}
$$`


In the previous section, we have shown that `$\frac{d {\gamma}_\tau(t) }{d t} \Big|_{t=0}$` is a  parametric representation of a Riemannian vector, which is a Riemannian gradient.
Notice that `$\nabla h_\lambda(\mathbf{\lambda}_0)$` is a Euclidean gradient. 

Now, we discuss the parameter transform between $\tau$ and $\lambda$, where we can express $\lambda$ in terms of $\tau$ denoted by $\lambda(\tau)$.


By the chain rule for the Euclidean gradient, we has
`$$
\begin{aligned}
(g_\tau)_i = \sum_{k}  \color{red}{ (g_\lambda)_k} \frac{\color{red}{ \partial \lambda^k(\tau) }}{ \partial \tau^i } 
\end{aligned}
\tag{6}\label{6}
$$` 


Let $J_{ki}:=\frac{\partial \lambda^k(\tau) }{ \partial \tau^i }$ denotes the $(k,i)$ entry of the Jacobian matrix. We can express the Jacobian matrix as below.

`$$
\begin{aligned}
\begin{matrix}
& \\
\mathbf{J} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    i=1 & i=2 \\ \hline
    J_{11} & J_{12}  \\
   J_{21} & J_{22} \\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12  \end{matrix} } \right )
    \begin{matrix}
    k=1 \\ k=2 
    \end{matrix}
\end{matrix}
\end{aligned}
$$`

Eq. `$\eqref{6}$` gives us the transformation rule for Eulcidean gradients as

`$$
\begin{aligned}
\mathbf{g}_\tau =  \mathbf{g}_\lambda \mathbf{J}
\end{aligned},
$$`
where `${\mathbf{g}}_\tau$` can be computed via a vector-Jacobian product in any standard Auto-Diff toolbox given that `${\mathbf{g}}_\lambda$` is pre-computed.



By Eq `$\eqref{5}$`, we obtain the transformation rule for Riemannian gradients as
`$$
\begin{aligned}
 \hat{\mathbf{g}}_\tau= \mathbf{J}^{-1}  \hat{\mathbf{g}}_\lambda 
\end{aligned}
$$` where $\mathbf{Q}:=\mathbf{J}^{-1}$ is also a Jacobian matrix and $Q_{ki}=\frac{\partial \tau^k(\lambda)}{\partial \lambda^i}$ is the $(k,i)$ entry of the matrix.

Note that `$\hat{\mathbf{g}}_\tau$` can be computed via a Jacobian-vector product used in forward-mode differentiation given that `$\hat{\mathbf{g}}_\lambda$` is pre-computed.


The elementwise expression of the transformation rule for Riemannian gradients  is
`$$
\begin{aligned}
(g_\tau)^k
 = \sum_{i}  \frac{ \partial \tau^k(\lambda)  }{\color{red} {\partial  \lambda^i} }  \color{red} {(g_\lambda)^i}
\end{aligned},
$$` 
Recall that these transformation rules are valid  when the Jacobian matrix is square and non-singular.


As we discussed in the previous post about [intrinsic parameterizations]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#intrinsic-parameterizations), the transformation map between $\tau$ and $\lambda$ must be bi-jective, which implies the Jacoabian matrix is square. 
Moreover, the map and its inverse map should be smooth, which implies that Jacobian matrix is well-defined and non-singular.


Finally, we give a transformation rule for the Fisher information matrix as defined at [the previous post]({{ site.baseurl }}{% post_url 2021-09-06-Geomopt01 %}#fisher-rao-metric).
We will use this transformation rule  to show that another **geometric property**---the length of a Riemannian vector---is preserved.

`$$
\begin{aligned}
 F_{ij}(\tau) := E_{p(w|\tau) }  [ \Big( \partial_{\tau_i} \log p(w|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau) \Big) ]
\end{aligned}
$$`
Since $ \log p(w|\tau )$ can be considered as a scalar function $h$ defined on the manifold for any valid $w$, we have 
`$$
\begin{aligned}
 \log p(w|\tau_0  ) = h_\tau(\tau_0) = h_\lambda(\lambda_0) =  \log p(w|\lambda_0  )
\end{aligned}
$$`

Thus, we have 
`$$
\begin{aligned}
 F_{ij}(\tau_0) &= E_{p(w|\tau_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ] \\
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
\end{aligned}
$$`

Recall that 
`$$
\begin{aligned}
\partial_{\tau_i} \log p(w|\tau_0 ) = \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 )
\end{aligned}
$$`
 
Moreover, the Jacobian matrix does not depent on $w$. Therefore, we have
`$$
\begin{aligned}
 F_{ij}(\tau_0) 
&=  E_{p(w|\lambda_0) }  [ \Big( \partial_{\tau_i} \log p(w|\tau_0 ) \Big)  \Big(\partial_{\tau_j} \log (w|\tau_0) \Big) ]\\
&=  E_{p(w|\lambda_0) }  [ \Big( \sum_k \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i } \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \sum_l \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  E_{p(w|\lambda_0) }  [ \Big(  \partial_{\lambda_k} \log p(w|\lambda_0 ) \Big)  \Big(  \partial_{\lambda_l} \log p(w|\lambda_0 ) \Big) ] \\
&= \sum_k \sum_l  \frac{ \partial \lambda^k(\tau_0) }{ \partial \tau^i }  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j } F_{kl}(\lambda_0)
\end{aligned}
$$`

We can re-express the above expression in a matrix form as

`$$
\begin{aligned}
\mathbf{F}_{\tau} (\tau_0) = \underbrace{\mathbf{J}^T}_{  \frac{ \partial \lambda^i(\tau_0) }{ \partial \tau^k }  } \mathbf{F}_{\lambda} (\lambda_0) \underbrace{\mathbf{J}}_{  \frac{ \partial \lambda^l(\tau_0) }{ \partial \tau^j }  } 
\end{aligned}
$$`

We can see that the length of a Riemannian vector is also invariant.
`$$
\begin{aligned}
\| \hat{\mathbf{g}}_\tau \|^2_F &= [\hat{\mathbf{g}}_\tau]^T \mathbf{F}_{\tau} (\tau_0) \hat{\mathbf{g}}_\tau \\
&= [\mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda]^T \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T [ \mathbf{J}^{-T}  \mathbf{F}_{\tau} (\tau_0) \mathbf{J}^{-1} ] \hat{\mathbf{g}}_\lambda \\
&= [\hat{\mathbf{g}}_\lambda]^T  \mathbf{F}_{\lambda} (\lambda_0)  \hat{\mathbf{g}}_\lambda = \| \hat{\mathbf{g}}_\lambda \|^2_F
\end{aligned}
$$`

Thus, the optimal solution of Eq. `$\eqref{2}$` under parametrization $\tau$ and $\lambda$ are equivalent since both the length and the directional derivative remain the same.
`$$
\begin{aligned}
(\nabla  f_\tau(\mathbf{\tau}_0)  )^T   \mathbf{v}^{(opt)}_{\tau}  &=   (\nabla  f_\lambda(\mathbf{\lambda}_0) )^T \mathbf{v}^{(opt)}_{\lambda} \\
\|  \mathbf{v}^{(opt)}_{\tau} \|^2_{F_\tau} & = \|  \mathbf{v}^{(opt)}_{\lambda} \|^2_{F_\lambda} \\
\mathbf{v}^{(opt)}_{\tau} & = \mathbf{J}^{-1}  \mathbf{v}^{(opt)}_{\lambda} \\
[\nabla  f_\tau(\mathbf{\tau}_0)]^T & =  [\nabla  f_\lambda(\mathbf{\lambda}_0)]^T \mathbf{J}
\end{aligned}.
$$` 

todo: show to GD case is not invariant and how to fix it. Connect to the invarinace of Newton's method

todo: the distance between two points is the manifold is not invariant unless we use the exact geodesic.




 
