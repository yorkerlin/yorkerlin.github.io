---
title: 'Statistical Manifolds, Basics of Information Geometry (part 1)'
date: 2021-09-06
permalink: /posts/2021/09/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand the Fisher-Rao metric also known as the Fisher information matrix (FIM).
* FIM plays an essential role in statistics and machine learning  
* FIM introduces geometric structures of a (parametric) distribution family

Motivation
------
Probabilistic modeling is a popular approach in machine learning by using distribution families.
The goal of information geometry is to study geometric structures (e.g., differentiable manifold structures) under a distribution family.

Example 1: given a set of features $X$ and labels $Y$, we can view a linear regression problem 
as 

$ \min_{\tau} \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2 = - \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i \| x_i^T\tau,1) + \mathrm{Constant} \approx - E_{p(x,y \| \tau)} [ \log p(x,y \| \tau) ]   $

where $ p(x,y \| \tau) = \mathcal{N}(y \| x^T\tau,1) p(x) $ and  $ \mathcal{N} (y \| x^T\tau,1) $ is a Gaussian distribution with mean $ x^T\tau $ and variance $ 1 $.

The **Fisher scoring algorithm** and the **emprical natural-gradient descent** can be derived from Example 1.



Example 2: given a prior $$ p(z) $$ and a likelihood $ p(\mathcal{D} \| z ) $ with observations $ \mathcal{D} $, we can view a variational inference problem with 
 an approximated distribution $ q(z \| \tau) $ as

$ \min_{\tau} \mathrm{KL} [ { q(z \| \tau) \|\| p( z \| \mathcal{D} ) }   ] = E_{q(z \| \tau)} [ \log q(z \| \tau) - \log p( z , \mathcal{D} )    ]  + \log p(\mathcal{D} )  $

where $ \mathrm{KL} [ \cdot \|\|  \cdot ] $ is the Kullbackâ€“Leibler divergence.

The **natural-gradient variatioal inference** and **natural evolution strategies** can be derived from Example 2.



Example 3: in reinforcement learning, we often use a policy distribution $ \pi(a \| \tau ) $ to generate actions.

The **natural policy gradient** method can be derived from Example 3.


We use a parametric family denoted by $ p(w\|\tau) $ in these examples. 
* In Example 1,  $ p(w\|\tau):=p(x,y\|\tau) $ and $ w $ is an observation. 
* In Example 2,  $ p(w\|\tau):=q(z\|\tau) $ and $ w $ is a latent variable. 
* In Example 3,  $ p(w\|\tau):=\pi(z\|\tau) $ and $ w $ is an action variable.


Statistical Manifolds
------

A statistical manifold is a distribution family with a differentiable manifold structure. 

Informally, a (smooth) manifold should be locally like a (flat) vector space. For example, consider the (curved) Earth surface looks like a (locally) flat space to us.  



We only consider parametric families with a Riemannian manifold structure in these blog posts.

Parametric families are important since they are
* widely used in machine learning
* well-studied and suitable for high-dimensional tasks compared to non-parametric counterparts such as Stein variational gradient methods
* flexible by using a mixture such as a Gaussian mixture



The following figure illustrates four distributions in a 1-dimensional Gaussian family denoted by
$ \\{ \mathcal{N}(w \|\mu,\sigma) \Big\| \mu \in \mathcal{R}, \sigma>0 \\}$, where $ p(w \| \tau ) = \mathcal{N}(w \|\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $  and $\tau :=(\mu,\sigma) $.

![Figure 2](/img/gauss1d.png "Source:Wikipedia")



Intrinsic Parameterizations
------
Unfortunately, we can not use an arbitrary differentiable parameterization to build a manifold structure in a parametric distribution family.
We require that  a manifold should be locally like a vector space denoted by $ E $ under a parameterization.

To illustrate this, let's consider a unit circle in $\mathcal{R}^2$ in the Figure.
<img src="/img/circle.png" alt="Source:Wikipedia" width="300"/>

Clearly, a point $ (0,1) $ highlighted in green is in the circle, where we consider its center as the origin.

Parametrization 1:

A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
$\\{ (t,\sqrt{1-t^2}) \| -0.1<t<0.1 \\} $.
We can similarly define a (local) parametrization at each point of the circle.

The manifold is (locally) flat since we can always find a small **1-dimensional** perturbation $E$ in the **1-dimensional** parameter space  of $\{ t \}$. 
This kind of parametrization is called an **intrinsic** parameterization.


Parametrization 2:

The circle does **not** look like a flat space under the following parametrization
$\\{ (x,y) \| x^2+y^2=1, x,y \in \mathcal{R} \\} $

The reason is that we cannot find a small **2-dimensional** perturbation $E$ in the **2-dimensional** parameter space of $\{ x\}$ and $\{ y \}$ due to the constraint $x^2+y^2=1$. 
This kind of parametrization is called an **extrinsic** parameterization.


(optional) Parametrization 3:

Technically, the following parametrization of the circle is **not** considered in the blog posts.
$\\{ (cos \theta,sin \theta ) \| \theta \in [0,2\pi)  \\} $

Let's define a map $f : [0,2\pi) \rightarrow \mathcal{S}^1 $, where we use $\mathcal{S}^1$ to denote the circle.
This map is bijective and smooth. However, its inverse map $f^{-1}$ is **not** continous at point $(0,1) \in  \mathcal{S}^1$.

We often have to introduce an additional smooth requirement of the inverse map when it comes to reparametrization (A.K.A. parameter transformation) for stastical manifolds.





Intrinsic Parameters for Statistical Manifolds
------
A regular condition to find an intrinsic parametrization is that the set of partial derivatives 
$ \\{ \partial_{\tau_i} \log p(w\|\tau) \\}  $ should be linearly independent for all $ w $. 
Note that this condition guarantees that the Fisher information matrix defined later is non-singular.

We will use two examples to illustrate this condition.


Example 1: 

In this example, we will show that the regular condition holds. Consider a 1-dimensional Gaussian family $ \\{ \mathcal{N}(w \|\mu,\sigma) \Big\| \mu \in \mathcal{R}, \sigma>0 \\}$ with parameter $\tau = (\mu,\sigma) $.
The  partial derivatives are

$ \partial_{\mu} \log \mathcal{N}(w \|\mu,\sigma) = \frac{w-\mu}{\sigma}, \,\,\, \partial_{\sigma} \log \mathcal{N}(w \|\mu,\sigma) = \frac{ (w-\mu)^2 }{2 \sigma^2} - \frac{1}{2\sigma} $

For simplicity,  let $\mu=0$ and $\sigma=1$. We can simplify the partial derivatives as below.
 
$ \partial_{\mu} \log \mathcal{N}(w \|\mu,\sigma) \Big\|\_{\mu=0,\sigma=1}= w, \,\,\, \partial_{\sigma} \log \mathcal{N}(w \|\mu,\sigma) \Big\|\_{\mu=0,\sigma=1} = \frac{ w^2 -1 }{2}  $


If $ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$ holds for any $w$, we have $c_1=c_2=0$, which implies  linear independence. Similarly, we can show that for any $\mu \in \mathcal{R}$ and $\sigma >0$, 
the partial derivatives are linearly independent.


Example 2: 

In this example, we will show that the regular condition fails. Consider a Bernoulli family $ \\{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big\| \pi_0>0, \pi_1>0  \\}$ with parameter $\tau = (\pi_0,\pi_1) $.
The partial derivatives are

$ \partial_{\pi_0} \log \mathcal{B}(w \|\pi_0, \pi_1) = \mathcal{B}(w \|\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   $

$ \partial_{\pi_1} \log \mathcal{B}(w \|\pi_0, \pi_1) = \mathcal{B}(w \|\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2}  $


Note that when $c_0 = \pi_0 \neq 0 $ and $c_1= \pi_1 \neq 0$, we have $c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$. Therefore, we can show that 
the partial derivatives are linearly dependent.

We will soon show that the regular condition also fails for Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$. This parameterization is indeed an extrinsic parameterization.
We will talk more about differences between an intrinsic parameterization and an extrinsic one at another blog post.


A remark is that the regular condition does hold for Bernoulli family $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big\| 0<\pi_0<1  \\}$ with parameter $\tau = \pi_0$.



Fisher-Rao Metric
------
When the regular condition holds, we can define the Fisher-Rao metric as below.
$ F_{ij}(\tau) := E_{p(w\|\tau) }  [ \Big( \partial_{\tau_i} \log p(w\|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w\|\tau) \Big) ]$.

We can express it in a matrix form as
$ F(\tau) := E_{p(w\|\tau) }  [ \Big( \nabla_{\tau} \log p(w\|\tau ) \Big)  \Big(\nabla_{\tau} \log (w\|\tau) \Big)^T ]$,
where $K$ is the length of parameter vector $\tau$ and 
$ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\tau_1} \log p(w\|\tau ), \cdots, \partial_{\tau_K} \log p(w\|\tau ) ]^T  $ is a column vector.
The matrix form is also known as the Fisher information matrix. 

We say a Riemannian metric is valid if it is positive definite everywhere in its parameter space.

The Fisher-Rao metric is
* ill-defined if the expectation does not exist
* a valid Riemannian metric if the regular condition holds and the expectation exists

In many cases, we could also compute the Fisher matrix as
$ F(\tau) := - E_{p(w\|\tau) }  [  \nabla_{\tau}^2 \log p(w\|\tau )  ]$.


Caveat of the Fisher matrix computation
------
There are some caveats when it comes to the Fisher matrix computation. 
As an example, consider Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$.
The following computation is not correct. Do you make similar mistakes like this?

Let $  p(w\|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$, where $\tau = (\pi_0,\pi_1)$. We have 
$  \nabla_{\tau} \log p(w\|\tau ) = \frac{1}{p(w\|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T $.

Thus, the Fisher matrix is
$ F(\tau) = E_{p(w\|\tau) } [ \frac{1}{p^2(w\|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\\\ 
\\mathcal{I}(w=0)  \mathcal{I}(w=1) &  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &  0 \\\\ 
0 &  \frac{1}{\pi_1} \end{bmatrix}  $

This computation is not correct. Do you know why it is not correct?

The key reason is the eqaulity constraint $ \pi_0+\pi_1=1 $.

By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint must be satisifed when we compute the Fisher matrix since the computation involves the expectation of this distribution.

Note that $ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\pi_0} \log p(w\|\tau ), \cdots, \partial_{\pi_1} \log p(w\|\tau ) ]^T $.

Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative $ \partial_{\pi_0} \log p(w\|\tau )$, we fix $\pi_1$ and allow $\pi_0$ to change.
However, given that $\pi_1$ is fixed and $ \pi_0 $ is fully determined by $\pi_1$ due to the eqaulity constraint $ \pi_0+\pi_1=1 $. 

Therefore, $  \partial_{\pi_0} \log p(w\|\tau ) $ is not well-defined.
In other words, the above Fisher matrix computation is not correct since $ \nabla_{\tau} \log p(w\|\tau ) $ does not exist.
 





(optional) Manifold with (Closed) Boundary
------
We define the dimension of a manifold by using the dimension of an intrinsic parametrization. Formally, we can show that any intrinsic parametrization of a manifold has the same degrees of freedom. 
We only illustrate this by examples.

The unit circle dicussed before is a 1-dimensional manifold.

<img src="/img/circle-org.png" alt="Source:Wikipedia" width="200"/>

The open unit ball is a 2-dimensional manifold.

<img src="/img/open-ball.png" alt="Source:Wikipedia" width="200"/>

Note that the closed unit ball is **not** a manifold. 

<img src="/img/closed-ball.png" alt="Source:Wikipedia" width="200"/>

The main reason is that its boundary  does not have the same degrees of freedom as its interior.

The closed ball is indeed a manifold with (closed) boundary. We will not consider this case in these blog posts.


