---
title: 'Statistical Manifolds, Basics of Information Geometry (part 1)'
date: 2021-09-06
permalink: /posts/2021/09/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Motivation
------
Probabilistic modeling is a popular approach in machine learning. The goal of information geometry is to study geometric structures (e.g., differentiable manifold structures) under a distribution family.

Example 1: given a set of features $x$ and labels $y$, we can view a linear regression problem 
as 

$$ \min_{\tau} - E_{p(x,y \| \tau)} [ \log p(x,y \| \tau) ] \approx  - \sum_{i}  \log \mathcal{N}(y_i \| x_i^T\tau,1) + \mathrm{Constant} $$

where $$ p(x,y \| \tau) = \mathcal{N}(y \| x^T\tau,1) p(x) $$ and  $$ \mathcal{N} (y \| x^T\tau,1) $$ is a Gaussian distribution with mean $$ x^T\tau $$ and variance $$ 1 $$.


Example 2: given a prior $$ p(z) $$ and a likelihood $$ p(\mathcal{D} \| z ) $$ with observations $$ \mathcal{D} $$, we can view a variational inference problem with 
 an approximated distribution $$  q(z \| \tau) $$ as

$$ \min_{\tau} \mathrm{KL} [ { q(z \| \tau), p( z \| \mathcal{D} ) }   ] = E_{q(z \| \tau)} [ \log q(z \| \tau) - \log p( z , \mathcal{D} )    ]  + \mathrm{Constant}  $$

where $$ \mathrm{KL} [ \cdot, \cdot ] $$ is the Kullbackâ€“Leibler divergence.


Example 3: in reinforcement learning, we often use a policy distribution $$ \pi(a \| \tau ) $$ to generate actions.


In these examples, we use a parametric family denoted by $$ p(w\|\tau) $$. 
* In Example 1,  $$ w $$ is an observation ( $ p(x,y) $ ). 
* In Example 2,  $$ w $$ is a latent variable ( $ q(z) $ ). 
* In Example 3,  $$ w $$ is an action variable ( $\pi (a) $ ).

Statistical Manifolds
------

A statistical manifold is a distribution family with a differentiable manifold structure. 
Informally, a (smooth) manifold should be locally like a Euclidean/flat space.

We only consider parametric families with a Riemannian manifold structure in these blog posts. 
The following figure illustrates four distributions in a 1-dimensional Gaussian family.
$$  p(w \| \mu, \sigma ) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $$

![Figure 1](/img/gaussian1d.png "Source:Wikipedia")

Parametric families are
* widely used in machine learning
* well-studied and suitable for high-dimensional tasks compared to non-parametric counterparts such as Stein variational gradient methods
* flexible by using a mixture such as a Gaussian mixture


Intrinsic Parameterizations
------


Fisher-Rao Metric
------



