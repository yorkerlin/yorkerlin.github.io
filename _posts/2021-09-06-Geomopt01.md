---
title: 'Statistical Manifolds, Basics of Information Geometry (part 1)'
date: 2021-09-06
permalink: /posts/2021/09/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Motivation
------
Probabilistic modeling is a popular approach in machine learning. The goal of information geometry is to study geometric structures (e.g., differentiable manifold structures) under a distribution family.

Example 1: given a set of features $X$ and labels $Y$, we can view a linear regression problem 
as 

$ \min_{\tau} \frac{1}{2n} \sum_{i}^{n}   (y_i-x_i^T\tau)^2 = - \frac{1}{n}  \sum_{i}^{n}  \log \mathcal{N}(y_i \| x_i^T\tau,1) + \mathrm{Constant} \approx - E_{p(x,y \| \tau)} [ \log p(x,y \| \tau) ]   $

where $ p(x,y \| \tau) = \mathcal{N}(y \| x^T\tau,1) p(x) $ and  $ \mathcal{N} (y \| x^T\tau,1) $ is a Gaussian distribution with mean $ x^T\tau $ and variance $ 1 $.


Example 2: given a prior $$ p(z) $$ and a likelihood $ p(\mathcal{D} \| z ) $ with observations $ \mathcal{D} $, we can view a variational inference problem with 
 an approximated distribution $ q(z \| \tau) $ as

$ \min_{\tau} \mathrm{KL} [ { q(z \| \tau) \|\| p( z \| \mathcal{D} ) }   ] = E_{q(z \| \tau)} [ \log q(z \| \tau) - \log p( z , \mathcal{D} )    ]  + \mathrm{Constant}  $

where $ \mathrm{KL} [ \cdot \|\|  \cdot ] $ is the Kullbackâ€“Leibler divergence.


Example 3: in reinforcement learning, we often use a policy distribution $ \pi(a \| \tau ) $ to generate actions.


In these examples, we use a parametric family denoted by $ p(w\|\tau) $. 
* In Example 1,  $ p(w\|\tau):=p(x,y\|\tau) $ and $ w $ is an observation. 
* In Example 2,  $ p(w\|\tau):=q(z\|\tau) $ and $ w $ is a latent variable. 
* In Example 3,  $ p(w\|\tau):=\pi(z\|\tau) $ and $ w $ is an action variable.

Statistical Manifolds
------

A statistical manifold is a distribution family with a differentiable manifold structure. 
Informally, a (smooth) manifold should be locally like a Euclidean/flat space.

We only consider parametric families with a Riemannian manifold structure in these blog posts. 
The following figure illustrates four distributions in a 1-dimensional Gaussian family.
$ \\{ \mathcal{N}(w \|\mu,\sigma) \Big| \mu \in \mathcal{R}, \sigma>0 \\}$, where $ p(w \| \tau ) = \mathcal{N}(w \|\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $  and $\tau :=(\mu,\sigma) $.

![Figure 1](/img/gauss1d.png "Source:Wikipedia")

Parametric families are
* widely used in machine learning
* well-studied and suitable for high-dimensional tasks compared to non-parametric counterparts such as Stein variational gradient methods
* flexible by using a mixture such as a Gaussian mixture


Intrinsic Parameterizations
------


Fisher-Rao Metric
------



