---
title: 'Statistical Manifolds, Basics of Information Geometry (part 1)'
date: 2021-09-06
permalink: /posts/2021/09/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Motivation
------
Probabilistic modeling is a popular approach in machine learning by using distribution families.
The goal of information geometry is to study geometric structures (e.g., differentiable manifold structures) under a distribution family.

Example 1: given a set of features $X$ and labels $Y$, we can view a linear regression problem 
as 

$ \min_{\tau} \frac{1}{2n} \sum_{i}^{n}   (y_i-x_i^T\tau)^2 = - \frac{1}{n}  \sum_{i}^{n}  \log \mathcal{N}(y_i \| x_i^T\tau,1) + \mathrm{Constant} \approx - E_{p(x,y \| \tau)} [ \log p(x,y \| \tau) ]   $

where $ p(x,y \| \tau) = \mathcal{N}(y \| x^T\tau,1) p(x) $ and  $ \mathcal{N} (y \| x^T\tau,1) $ is a Gaussian distribution with mean $ x^T\tau $ and variance $ 1 $.


Example 2: given a prior $$ p(z) $$ and a likelihood $ p(\mathcal{D} \| z ) $ with observations $ \mathcal{D} $, we can view a variational inference problem with 
 an approximated distribution $ q(z \| \tau) $ as

$ \min_{\tau} \mathrm{KL} [ { q(z \| \tau) \|\| p( z \| \mathcal{D} ) }   ] = E_{q(z \| \tau)} [ \log q(z \| \tau) - \log p( z , \mathcal{D} )    ]  + \log p(\mathcal{D} )  $

where $ \mathrm{KL} [ \cdot \|\|  \cdot ] $ is the Kullbackâ€“Leibler divergence.


Example 3: in reinforcement learning, we often use a policy distribution $ \pi(a \| \tau ) $ to generate actions.


We use a parametric family denoted by $ p(w\|\tau) $ in these examples. 
* In Example 1,  $ p(w\|\tau):=p(x,y\|\tau) $ and $ w $ is an observation. 
* In Example 2,  $ p(w\|\tau):=q(z\|\tau) $ and $ w $ is a latent variable. 
* In Example 3,  $ p(w\|\tau):=\pi(z\|\tau) $ and $ w $ is an action variable.


Statistical Manifolds
------

A statistical manifold is a distribution family with a differentiable manifold structure. 
Informally, a (smooth) manifold should be locally like a Euclidean/flat space.

We only consider parametric families with a Riemannian manifold structure in these blog posts.

Parametric families are
* widely used in machine learning
* well-studied and suitable for high-dimensional tasks compared to non-parametric counterparts such as Stein variational gradient methods
* flexible by using a mixture such as a Gaussian mixture



The following figure illustrates four distributions in a 1-dimensional Gaussian family denoted by
$ \\{ \mathcal{N}(w \|\mu,\sigma) \Big\| \mu \in \mathcal{R}, \sigma>0 \\}$, where $ p(w \| \tau ) = \mathcal{N}(w \|\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $  and $\tau :=(\mu,\sigma) $.

![Figure 1](/img/gauss1d.png "Source:Wikipedia")



Intrinsic Parameterizations
------
Unfortunately, we can not use an arbitrary differentiable parameterization to build a manifold structure in a parametric distribution family.
We assume that a parametric family should be locally like a flat space under a parameterization.


A regular condition related to the assumption is that the set of partial derivatives 
$ \\{ \partial_{\tau_i} \log p(w\|\tau) \\}  $ should be linearly independent for all $ w $. 
Note that this condition guarantees that the Fisher-Rao metric is non-degenerative. 

We will use two examples to illustrate this condition.


Example 1: In this example, we will show that the regular condition holds. Consider a 1-dimensional Gaussian family $ \\{ \mathcal{N}(w \|\mu,\sigma) \Big\| \mu \in \mathcal{R}, \sigma>0 \\}$ with parameter $\tau = (\mu,\sigma) $.
The  partial derivatives are

$ \partial_{\mu} \log \mathcal{N}(w \|\mu,\sigma) = \frac{w-\mu}{\sigma}, \,\,\, \partial_{\sigma} \log \mathcal{N}(w \|\mu,\sigma) = \frac{ (w-\mu)^2 }{2 \sigma^2} - \frac{1}{2\sigma} $

For simplicity,  let $\mu=0$ and $\sigma=1$. We can simplify the partial derivatives as below.
 
$ \partial_{\mu} \log \mathcal{N}(w \|\mu,\sigma) \Big\|\_{\mu=0,\sigma=1}= w, \,\,\, \partial_{\sigma} \log \mathcal{N}(w \|\mu,\sigma) \Big\|\_{\mu=0,\sigma=1} = \frac{ w^2 -1 }{2}  $


If $ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$ holds for any $w$, we have $c_1=c_2=0$, which implies  linear independence. Similarly, we can show that for any $\mu \in \mathcal{R}$ and $\sigma >0$, 
the partial derivatives are linearly independent.


Example 2: In this example, we will show that the regular condition fails. Consider a Bernoulli family $ \\{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big\| \pi_0>0, \pi_1>0  \\}$ with parameter $\tau = (\pi_0,\pi_1) $.
The partial derivatives are

$ \partial_{\pi_0} \log \mathcal{B}(w \|\pi_0, \pi_1) = \mathcal{B}(w \|\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   $

$ \partial_{\pi_1} \log \mathcal{B}(w \|\pi_0, \pi_1) = \mathcal{B}(w \|\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2}  $


Note that when $c_0 = \pi_0 \neq 0 $ and $c_1= \pi_1 \neq 0$, we have $c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$. Therefore, we can show that 
the partial derivatives are linearly dependent.

We will soon show that the regular condition also fails for Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$. This parameterization is indeed an extrinsic parameterization. We will talk more about differences between an intrinsic parameterization and an extrinsic one at another blog post.


A remark is that the regular condition does hold for Bernoulli family $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big\| 0<\pi_0<1  \\}$ with parameter $\tau = \pi_0$.



Fisher-Rao Metric
------
When the regular condition holds, we can define the Fisher-Rao metric as below.
$ F_{ij}(\tau) := E_{p(w\|\tau) }  [ \Big( \partial_{\tau_i} \log p(w\|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w\|\tau) \Big) ]$.

We can express it in a matrix form as
$ F(\tau) := E_{p(w\|\tau) }  [ \Big( \nabla_{\tau} \log p(w\|\tau ) \Big)  \Big(\nabla_{\tau} \log (w\|\tau) \Big)^T ]$,
where $K$ is the length of parameter vector $\tau$ and 
$ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\tau_1} \log p(w\|\tau ), \cdots, \partial_{\tau_K} \log p(w\|\tau ) ]^T  $ is a column vector.
The matrix form is also known as the Fisher information matrix. 

We say a Riemannian metric is valid if it is positive definite everywhere in its parameter space.

The Fisher-Rao metric is
* ill-defined if the expectation does not exist
* a valid Riemannian metric if the regular condition holds and the expectation exists

In many cases, we could also compute the Fisher matrix as
$ F(\tau) := - E_{p(w\|\tau) }  [  \nabla_{\tau}^2 \log p(w\|\tau )  ]$.


There are some caveats when it comes to the Fisher matrix computation. 
As an example, consider Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$.
The following computation is not correct. Do you make similar mistakes like this?

Let $  p(w\|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$, where $\tau = (\pi_0,\pi_1)$. We have 
$  \nabla_{\tau} \log p(w\|\tau ) = \frac{1}{p(w\|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T $.

Thus, the Fisher matrix is
$ F(\tau) = E_{p(w\|\tau) } [ \frac{1}{p^2(w\|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\\\ 
\\mathcal{I}(w=0)  \mathcal{I}(w=1) &  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &  0 \\\\ 
0 &  \frac{1}{\pi_1} \end{bmatrix}  $

This computation is not correct. Do you know why it is not correct?

The key reason is the eqaulity constraint $ \pi_0+\pi_1=1 $. By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint must be satisifed when we compute the Fisher matrix since the matrix involves the expectation of this distribution.
Note that $ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\pi_0} \log p(w\|\tau ), \cdots, \partial_{\pi_1} \log p(w\|\tau ) ]^T $.
Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative $ \partial_{\pi_0} \log p(w\|\tau )$, we fix $\pi_1$ and allow $\pi_0$ to change.
However, given that $\pi_1$ is fixed and $ \pi_0 $ is fully determined by $\pi_1$ due to the eqaulity constraint $ \pi_0+\pi_1=1 $. Therefore, $  \partial_{\pi_0} \log p(w\|\tau ) $ is not well-defined.
In other words, the above Fisher matrix computation is not correct since $ \nabla_{\tau} \log p(w\|\tau ) $ does not exist.
 










 




