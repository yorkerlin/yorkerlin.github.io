---
title: 'Statistical Manifolds, Basics of Information Geometry (part 1)'
date: 2021-09-06
permalink: /posts/2021/09/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

Goal
------
This blog post should help readers to understand the Fisher-Rao metric also known as the Fisher information matrix (FIM).
* FIM plays an essential role in statistics and machine learning  
* FIM introduces geometric structures of a (parametric) distribution family

Motivation
------
Probabilistic modeling is a popular approach in machine learning by using distribution families.
We can exploit the (hidden) geometric structures induced by the Fisher-Rao metric.
Applications of the metric in machine learning are natural-gradient methods as shown in the following examples.

Example 1: 

given a set of features $X$ and labels $Y$, we can view a linear regression problem 
as 

$ \min_{\tau} \frac{1}{2n} \sum_{i=1}^{n}   (y_i-x_i^T\tau)^2 = - \frac{1}{n}  \sum_{i=1}^{n}  \log \mathcal{N}(y_i \| x_i^T\tau,1) + \mathrm{Constant} 
\approx - E_{p(x,y \| \tau)} [ \log p(x,y \| \tau) ]   $

where $ p(x,y \| \tau) = \mathcal{N}(y \| x^T\tau,1) p(x) $ and  $ \mathcal{N} (y \| x^T\tau,1) $ is a Gaussian distribution with mean $ x^T\tau $ and variance $ 1 $.

The **Fisher scoring algorithm** and the **emprical natural-gradient descent** can be derived from Example 1.



Example 2: 

given a prior $$ p(z) $$ and a likelihood $ p(\mathcal{D} \| z ) $ with observations $ \mathcal{D} $, we can view a variational inference problem with 
 an approximated distribution $ q(z \| \tau) $ as

$ \min_{\tau} \mathrm{KL} [ { q(z \| \tau) \|\| p( z \| \mathcal{D} ) }   ] = E_{q(z \| \tau)} [ \log q(z \| \tau) - \log p( z , \mathcal{D} )    ]  + \log p(\mathcal{D} )  $

where $ \mathrm{KL} [ \cdot \|\|  \cdot ] $ is the Kullbackâ€“Leibler divergence.

The **natural-gradient variatioal inference** and **natural evolution strategies** can be derived from Example 2.



Example 3: 

in reinforcement learning, we often use a policy distribution $ \pi(a \| \tau ) $ to generate actions.

The **natural policy gradient** method can be derived from Example 3.


In these examples, we use the Fisher-Rao metric to build a manifold structure on a parametric family denoted by $ p(w\|\tau) $. 
* In Example 1,  $ p(w\|\tau):=p(x,y\|\tau) $ and $ w $ is an observation. 
* In Example 2,  $ p(w\|\tau):=q(z\|\tau) $ and $ w $ is a latent variable. 
* In Example 3,  $ p(w\|\tau):=\pi(a\|\tau) $ and $ w $ is an action variable.


Statistical Manifolds
------
A statistical manifold is a distribution family with a differentiable manifold structure. 
We only consider parametric families with a Riemannian manifold structure induced by the Fisher-Rao metric in these blog posts.

Informally, a (smooth) manifold should be locally like a (flat) vector space. For example, consider the (curved) Earth surface looks like a (locally) flat space to us.


Parametric families are important since they are
* widely used in machine learning
* well-studied and suitable for high-dimensional tasks compared to non-parametric counterparts such as Stein variational gradient methods
* flexible by using a mixture such as a Gaussian mixture


The following figure illustrates four distributions in a 1-dimensional Gaussian family denoted by
$ \\{ \mathcal{N}(w \|\mu,\sigma) \Big\| \mu \in \mathcal{R}, \sigma>0 \\}$, where $ p(w \| \tau ) = \mathcal{N}(w \|\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma} } \exp [- \frac{(w-\mu)^2}{2\sigma} ] $  and $\tau :=(\mu,\sigma) $.

![Figure 2](/img/gauss1d.png "Source:Wikipedia")



Intrinsic Parameterizations
------
Unfortunately, we can not use an arbitrary differentiable parameterization to build a manifold structure in a parametric distribution family.
We require that  a manifold should be locally like a vector space denoted by $ E $ under a parameterization.
In other words, we should allow a small perturbation $ E $ such as an open ball in a given parameter space.

To illustrate this, let's consider a unit circle in $\mathcal{R}^2$ as shown in the Figure.
Clearly, a point $ (0,1) $ highlighted in green is in the circle, where we consider its center as the origin.



Parametrization 1:

A (local) parametrization at $ (0,1) $ highlighted in red for the circle is
$\\{ (t,\sqrt{1-t^2}) \| -h<t<h \\} $, where $h=0.1$.
<img src="/img/circle.png" title="Source:Wikipedia" width="300"/>

We can similarly define a (local) parametrization at each point of the circle. In fact, we only need to use  four (local) parameterizations to represent the circle as shown below.

<img src="/img/charts.png" title="Source:Wikipedia" width="200"/>



The manifold is (locally) flat since we can always find a small **1-dimensional** perturbation $E$ in the **1-dimensional** parameter space  of $\{ t \}$. 
This kind of parametrization is called an **intrinsic** parameterization.

<img src="/img/1d-perturbation.png" title="Fig" width="300"/>



Parametrization 2:

The circle does **not** look like a flat space under the following parametrization
$\\{ (x,y) \| x^2+y^2=1, x,y \in \mathcal{R} \\} $

The reason is that we cannot find a small **2-dimensional** perturbation $E$ in the **2-dimensional** parameter space of $\{ x\}$ and $\{ y \}$ due to the constraint $x^2+y^2=1$. 
This kind of parametrization is called an **extrinsic** parameterization.

<img src="/img/2d-perturbation.png" title="Fig" width="300"/>

(optional) Parametrization 3:

Technically, the following parametrization of the circle is **not** considered in the blog posts.
$\\{ (\sin \theta, \cos \theta ) \| \theta \in [0,2\pi)  \\} $

Let's define a map $f : [0,2\pi) \rightarrow \mathcal{S}^1 $, where we use $\mathcal{S}^1$ to denote the circle.
This map is bijective and smooth. However, its inverse map $f^{-1}$ is **not** continous at point $(0,1) \in  \mathcal{S}^1$.

This smooth requirement of the inverse map is essential when it comes to reparametrization (A.K.A. parameter transformation) for stastical manifolds.




Intrinsic Parameters for Statistical Manifolds
------
A regularity condition to find an intrinsic parametrization is that the set of partial derivatives 
$ \\{ \partial_{\tau_i} \log p(w\|\tau) \\}  $ should be linearly independent for all $ w $.

Note that when we compute the  partial derivative w.r.t. parameter $ \tau_i $, we consider $ w $ to be fixed.

We will use two examples to illustrate this condition.


Example 1: 

In this example, we will show that the regular condition holds. Consider a 1-dimensional Gaussian family $ \\{ \mathcal{N}(w \|\mu,\sigma) \Big\| \mu \in \mathcal{R}, \sigma>0 \\}$ with parameter $\tau = (\mu,\sigma) $.
The  partial derivatives are

$ \partial_{\mu} \log \mathcal{N}(w \|\mu,\sigma) = \frac{w-\mu}{\sigma}, \,\,\, \partial_{\sigma} \log \mathcal{N}(w \|\mu,\sigma) = \frac{ (w-\mu)^2 }{2 \sigma^2} - \frac{1}{2\sigma} $

For simplicity,  let $\mu=0$ and $\sigma=1$. We can simplify the partial derivatives as below.
 
$ \partial_{\mu} \log \mathcal{N}(w \|\mu,\sigma) \Big\|\_{\mu=0,\sigma=1}= w, \,\,\, \partial_{\sigma} \log \mathcal{N}(w \|\mu,\sigma) \Big\|\_{\mu=0,\sigma=1} = \frac{ w^2 -1 }{2}  $


If $ c_1 w + c_2  (\frac{ w^2 -1 }{2}) = 0$ holds for any $w$, we have $c_1=c_2=0$, which implies  linear independence. Similarly, we can show that for any $\mu \in \mathcal{R}$ and $\sigma >0$, 
the partial derivatives are linearly independent.


Example 2: 

In this example, we will show that the regular condition fails. Consider a Bernoulli family $ \\{ \mathcal{I}(w=0) \frac{\pi_0}{\pi_0+\pi_1} + \mathcal{I}(w=1) \frac{\pi_1}{\pi_0+\pi_1} \Big\| \pi_0>0, \pi_1>0  \\}$ with parameter $\tau = (\pi_0,\pi_1) $.
The partial derivatives are

$ \partial_{\pi_0} \log \mathcal{B}(w \|\pi_0, \pi_1) = \mathcal{B}(w \|\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{\pi_1}{(\pi_0+\pi_1)^2}   $

$ \partial_{\pi_1} \log \mathcal{B}(w \|\pi_0, \pi_1) = \mathcal{B}(w \|\pi_0, \pi_1)  (\mathcal{I}(w=0)-\mathcal{I}(w=1)) \frac{-\pi_0}{ (\pi_0+\pi_1 )^2}  $


Note that when $c_0 = \pi_0 \neq 0 $ and $c_1= \pi_1 \neq 0$, we have $c_0 \frac{\pi_1}{(\pi_0+\pi_1)^2}  + c_1 \frac{-\pi_0}{ (\pi_0+\pi_1 )^2} = 0$. Therefore, we can show that 
the partial derivatives are linearly dependent.

We will soon show that the regular condition also fails for Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$. This parameterization is indeed an extrinsic parameterization as mentioned before.


A remark is that the regular condition does hold for Bernoulli family $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) (1-\pi_0)  \Big\| 0<\pi_0<1  \\}$ with parameter $\tau = \pi_0$.



Fisher-Rao Metric
------
If a parameterization satisfies the regularity condition, we can define the Fisher-Rao metric under this parameterization as:
$ F_{ij}(\tau) := E_{p(w\|\tau) }  [ \Big( \partial_{\tau_i} \log p(w\|\tau ) \Big)  \Big(\partial_{\tau_j} \log (w\|\tau) \Big) ]$.
Note that the metric could be ill-defined since the expectation may not exist.

We can express the metric in a matrix form as
$ F(\tau) := E_{p(w\|\tau) }  [ \Big( \nabla_{\tau} \log p(w\|\tau ) \Big)  \Big(\nabla_{\tau} \log (w\|\tau) \Big)^T ]$,
where $K$ is the length of parameter vector $\tau$ and 
$ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\tau_1} \log p(w\|\tau ), \cdots, \partial_{\tau_K} \log p(w\|\tau ) ]^T  $ is a column vector.
The matrix form is also known as the Fisher information matrix. 

The regularity condition guarantees that the Fisher information matrix is non-singular.
The condition also is related to the model identification in maximum likelihood estimation.


The Fisher-Rao metric is a valid Riemannian metric since it is positive definite everywhere in an **intrinsic** parameter space.
Recall that when we perform a parameter transformation, we require the both the transformation map and its inverse map to be smooth.
If this smoothness requirement is statistified, we can show the Fisher-Rao metric is a Riemannian metric for any **intrinsic** parameterization.
The regularity condition gives us a way to find an **intrinsic** parameterization.


In many cases, we could also compute the Fisher matrix as
$ F(\tau) := - E_{p(w\|\tau) }  [  \nabla_{\tau}^2 \log p(w\|\tau )  ]$.



Caveat of the Fisher matrix computation
------
There are some caveats when it comes to the Fisher matrix computation. In particular, the Fisher matrix may not be well-defined and the regularity condition should be satisfied. 

As an example, consider Bernoulli family  $ \\{ \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1  \Big\| \pi_0>0, \pi_1>0, \pi_0+\pi_1=1  \\}$ with parameter $\tau = (\pi_0,\pi_1)$.
The following computation is not correct. Do you make similar mistakes like this?

Let $  p(w\|\tau ) =  \mathcal{I}(w=0) \pi_0+ \mathcal{I}(w=1) \pi_1$, where $\tau = (\pi_0,\pi_1)$. We have 
$  \nabla_{\tau} \log p(w\|\tau ) = \frac{1}{p(w\|\tau )} [\mathcal{I}(w=0) ,  \mathcal{I}(w=1)]^T $.

Thus, the Fisher matrix is
$ F(\tau) = E_{p(w\|\tau) } [ \frac{1}{p^2(w\|\tau)} \begin{bmatrix} \mathcal{I}^2(w=0) &  \mathcal{I}(w=1)  \mathcal{I}(w=0) \\\\ 
\\mathcal{I}(w=0)  \mathcal{I}(w=1) &  \mathcal{I}^2(w=1) \end{bmatrix} ]   = \begin{bmatrix} \frac{1}{\pi_0} &  0 \\\\ 
0 &  \frac{1}{\pi_1} \end{bmatrix}  $

This computation is not correct. Do you know why it is not correct?

The key reason is the equality constraint $ \pi_0+\pi_1=1 $.

By definition, a Bernoulli distribution is valid only when the constraint holds. 
Thus, the constraint must be satisifed when we compute the Fisher matrix since the computation involves the expectation of this distribution.

Note that $ \nabla_{\tau} \log p(w\|\tau ) := [ \partial_{\pi_0} \log p(w\|\tau ), \cdots, \partial_{\pi_1} \log p(w\|\tau ) ]^T $.

Unfortunately, these partial derivatives do not exist. By the definition of  partial derivative $ \partial_{\pi_0} \log p(w\|\tau )$, we fix $\pi_1$ and allow $\pi_0$ to change.
However, given that $\pi_1$ is fixed and $ \pi_0 $ is fully determined by $\pi_1$ due to the equality constraint $ \pi_0+\pi_1=1 $. 

Therefore, $  \partial_{\pi_0} \log p(w\|\tau ) $ is not well-defined.
In other words, the above Fisher matrix computation is not correct since $ \nabla_{\tau} \log p(w\|\tau ) $ does not exist.
 





(optional) Manifolds with (Closed) Boundary
------
We define the dimension of a manifold by using the dimension of an intrinsic parametrization. Formally, we can show that any intrinsic parametrization of a manifold has the same degrees of freedom. 
We now illustrate this by examples.

The unit circle dicussed before is a 1-dimensional manifold.

<img src="/img/circle-org.png" alt="Source:Wikipedia" width="200"/>

The open unit ball is a 2-dimensional manifold.

<img src="/img/open-ball.png" alt="Source:Wikipedia" width="200"/>

Note that the closed unit ball is **not** a manifold. 

<img src="/img/closed-ball.png" alt="Source:Wikipedia" width="200"/>

The main reason is that its boundary  does not have the same degrees of freedom as its interior.

The closed ball is indeed a manifold with (closed) boundary. We will not consider this case in these blog posts.


