---
title: 'Natural Gradient Descent'
date: 2021-08-19
permalink: /posts/2021/08/Geomopt01/
tags:
  - Natural Gradient Descent
  - Information Geometry
  - Riemannian Manifold
---

In this serise of blog posts, I will cover the basics of natural-gradient descent (NGD).


Introduction to Natural-gradient Descent
======

<!--Aren't headings cool?-->
<!---------->

Recall that in gradient descent (GD), we use the steepest (Euclidean) descent direction w.r.t. the Euclidean metric (under the Cartesian coordinate).

Simiarly, natural-gradient descent (NGD) is a gradient-based method with the steepest descent direction w.r.t. the Fisher-Rao metric.
The metric often is defined for (parametric) probability distribution spaces.


Clearly, the usefulness of NGD depends on the Fisher-Rao metric. The metric has the following nice properties.

* In (parametric) probability distribution spaces, the Fisher-Rao metric plays an
essential role since the Fisher-Rao metric is associated with maximum-likelihood estimates. Many machine learning
applications are essentially about maximum-likelihood 

* NGD is less sensitive than GD when it comes to parametrizatoin transforms thanks to the metric.
We often perform a parametrization transform to get rid of parameterization constraints. Recall that Newton's method is invariant under invertible linear transformations while GD is not. 
Later, we will see that NGD is also like Newton's method. 

Before we define the Fisher-Rao metric, we have to first define a (parametric) distribution space.
